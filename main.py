# -*- coding: utf-8 -*-
"""USD_JYP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J_9Eb69kvOu-RUMDxXgb9DcbR2QRIYCs

# Import Dataset From Kaggle

###Import Kaggle JSON
"""

!pip install -q kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""###Import Dataset and Unzip"""

!kaggle datasets download -d cfchan/daily-usdjpy-20002019-with-technical-indicators
!unzip "/content/daily-usdjpy-20002019-with-technical-indicators.zip"

"""#Import Library"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

"""#Membaca Data"""

df = pd.read_csv('/content/USDJPY_Daily.csv')
df.head()

print("jumlah data = {}\n".format(df.shape[0]))
df.isnull().sum()

data_usd_jpy = df.iloc[:, 2:3].values

plt.figure(figsize=(15,5))
plt.plot(data_usd_jpy)
plt.title('USD JPY time series',
          fontsize=20);

"""#Pra-pemrosesan

##Membagi dataset
- 5000 data dilakukan pembagian dengan 80% untuk training dan 20% untuk validasi
- 757 data digunakan untuk testing
"""

data_train_val = data_usd_jpy[:5000] #data train dan validasi
data_test = data_usd_jpy[5000-60:] #data test dengan mengambil 60 data sebelumnya
data_test_asli = data_usd_jpy[5000:] #data test asli

dataTrain = data_train_val[:len(data_train_val)-int(len(data_train_val)*0.2),]
dataVal = data_train_val[len(data_train_val)-int(len(data_train_val)*0.2)-60:,]
dataTest = data_test

print(dataTrain.shape)
print(dataVal.shape)
print(dataTest.shape)

"""##Transform Data"""

from sklearn.preprocessing import MinMaxScaler

sc = MinMaxScaler(feature_range = (0, 1))
train_scaled = sc.fit_transform(dataTrain)
val_scaled = sc.fit_transform(dataVal)
print(train_scaled.shape, val_scaled.shape)

"""##Pengamatan: menggunakan 60 data sebelumnya (interval) untuk menghitung nilai prediksi"""

def get_scaled_data(interval, data):
    X, Y = [], []
    for i in range(interval, len(data)):
        X.append(data[i-interval:i, 0])
        Y.append(data[i, 0])
    X, Y = np.array(X), np.array(Y)
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    return X, Y

X_train, y_train = get_scaled_data(60, train_scaled)
X_val, y_val = get_scaled_data(60, val_scaled)

"""#Model LSTM"""

LSTM_model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(units=100, return_sequences=True, input_shape=(X_train.shape[1], 1)),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.LSTM(units=50),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(units=1)
])

LSTM_model.compile(loss= 'mean_squared_error',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999),
              metrics=['accuracy'])

mcp_save = tf.keras.callbacks.ModelCheckpoint("best_model.hdf5", monitor='loss', mode='min', verbose=1)

history = LSTM_model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val), 
                         verbose=1, callbacks=[mcp_save])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""# Prediksi"""

dataTest_sc = sc.transform(dataTest)
print(dataTest_sc.shape)

X_test = []

for i in range(60, len(dataTest_sc)):
    X_test.append(dataTest_sc[i-60:i, 0])

X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

print(X_test.shape)

prediksi = LSTM_model.predict(X_test)
prediksi = sc.inverse_transform(prediksi)

plt.plot(prediksi,"r",label="Nilai Prediksi")
plt.plot(data_test_asli,"b",label="Nilai Real")
plt.legend()
plt.title(' Prediction')
plt.xlabel('Time')
plt.show()

"""# MAE"""

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(data_test_asli, prediksi)
print(mae)

maxData = max(data_usd_jpy)
minData = min(data_usd_jpy)
MAE = (maxData - minData) * 0.1
print(maxData, minData, MAE)